{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/trxxlxrd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/trxxlxrd/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 0/100 iterations\n",
      "Done: 10/100 iterations\n",
      "Done: 20/100 iterations\n",
      "Done: 30/100 iterations\n",
      "Done: 40/100 iterations\n",
      "Done: 50/100 iterations\n",
      "Done: 60/100 iterations\n",
      "Done: 70/100 iterations\n",
      "Done: 80/100 iterations\n",
      "Done: 90/100 iterations\n",
      "Topic #1: since, cause, different, difference, point, less, many, actually, likely, find\n",
      "Topic #2: one, would, also, like, go, think, good, even, thing, take\n",
      "Topic #3: time, back, went, said, took, day, got, told, put, next\n",
      "Topic #4: would, one, make, take, go, like, think, seems, people, know\n",
      "Topic #5: space, university, research, new, april, information, science, center, year, office\n",
      "Topic #6: state, law, government, right, public, american, gun, people, federal, crime\n",
      "Topic #7: would, one, think, like, see, really, people, good, make, get\n",
      "Topic #8: car, good, new, get, much, price, buy, pay, cost, need\n",
      "Topic #9: get, like, think, one, would, going, know, pretty, want, even\n",
      "Topic #10: one, get, like, would, know, look, might, could, bit, got\n",
      "Topic #11: anyone, please, know, thanks, would, could, looking, post, anybody, send\n",
      "Topic #12: think, know, make, never, sure, want, right, would, one, something\n",
      "Topic #13: key, system, use, chip, used, using, phone, encryption, clipper, two\n",
      "Topic #14: people, world, child, many, war, two, state, woman, attack, year\n",
      "Topic #15: would, like, one, people, get, know, someone, thing, something, want\n",
      "Topic #16: god, believe, christian, say, people, must, jesus, word, bible, human\n",
      "Topic #17: game, last, team, year, first, play, player, win, second, go\n",
      "Topic #18: drive, card, use, system, hard, video, disk, work, computer, need\n",
      "Topic #19: window, using, file, use, run, program, running, problem, get, set\n",
      "Topic #20: list, send, number, available, via, following, include, information, copy, posted\n",
      "Topic #1: sci.med (1101 documents)\n",
      "Top words: since, cause, different, difference, point, less, many, actually, likely, find\n",
      "\n",
      "Topic #2: rec.motorcycles (615 documents)\n",
      "Top words: one, would, also, like, go, think, good, even, thing, take\n",
      "\n",
      "Topic #3: rec.autos (600 documents)\n",
      "Top words: time, back, went, said, took, day, got, told, put, next\n",
      "\n",
      "Topic #4: alt.atheism (566 documents)\n",
      "Top words: would, one, make, take, go, like, think, seems, people, know\n",
      "\n",
      "Topic #5: sci.space (518 documents)\n",
      "Top words: space, university, research, new, april, information, science, center, year, office\n",
      "\n",
      "Topic #6: talk.politics.guns (563 documents)\n",
      "Top words: state, law, government, right, public, american, gun, people, federal, crime\n",
      "\n",
      "Topic #7: soc.religion.christian (544 documents)\n",
      "Top words: would, one, think, like, see, really, people, good, make, get\n",
      "\n",
      "Topic #8: misc.forsale (567 documents)\n",
      "Top words: car, good, new, get, much, price, buy, pay, cost, need\n",
      "\n",
      "Topic #9: sci.electronics (403 documents)\n",
      "Top words: get, like, think, one, would, going, know, pretty, want, even\n",
      "\n",
      "Topic #10: rec.sport.baseball (416 documents)\n",
      "Top words: one, get, like, would, know, look, might, could, bit, got\n",
      "\n",
      "Topic #11: comp.graphics (851 documents)\n",
      "Top words: anyone, please, know, thanks, would, could, looking, post, anybody, send\n",
      "\n",
      "Topic #12: rec.sport.hockey (387 documents)\n",
      "Top words: think, know, make, never, sure, want, right, would, one, something\n",
      "\n",
      "Topic #13: sci.crypt (415 documents)\n",
      "Top words: key, system, use, chip, used, using, phone, encryption, clipper, two\n",
      "\n",
      "Topic #14: talk.politics.mideast (452 documents)\n",
      "Top words: people, world, child, many, war, two, state, woman, attack, year\n",
      "\n",
      "Topic #15: talk.politics.misc (336 documents)\n",
      "Top words: would, like, one, people, get, know, someone, thing, something, want\n",
      "\n",
      "Topic #16: talk.religion.misc (671 documents)\n",
      "Top words: god, believe, christian, say, people, must, jesus, word, bible, human\n",
      "\n",
      "Topic #17: comp.os.ms-windows.misc (600 documents)\n",
      "Top words: game, last, team, year, first, play, player, win, second, go\n",
      "\n",
      "Topic #18: comp.sys.ibm.pc.hardware (688 documents)\n",
      "Top words: drive, card, use, system, hard, video, disk, work, computer, need\n",
      "\n",
      "Topic #19: comp.windows.x (678 documents)\n",
      "Top words: window, using, file, use, run, program, running, problem, get, set\n",
      "\n",
      "Topic #20: comp.sys.mac.hardware (343 documents)\n",
      "Top words: list, send, number, available, via, following, include, information, copy, posted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "documents = newsgroups_train.data\n",
    "labels = newsgroups_train.target\n",
    "label_names = newsgroups_train.target_names\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(doc):\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in doc.split() if word.isalpha() and word.lower() not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "X = vectorizer.fit_transform(preprocessed_documents)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "n_topics = 20\n",
    "alpha = 1.0\n",
    "beta = 1.0\n",
    "n_iterations = 100\n",
    "\n",
    "n_docs, n_words = X.shape\n",
    "word_topic_counts = np.zeros((n_words, n_topics))\n",
    "doc_topic_counts = np.zeros((n_docs, n_topics))\n",
    "topic_totals = np.zeros(n_topics)\n",
    "doc_topic_assignments = []\n",
    "\n",
    "for d in range(n_docs):\n",
    "    doc = X[d].indices\n",
    "    topics = []\n",
    "    for w in doc:\n",
    "        topic = random.randint(0, n_topics - 1)\n",
    "        word_topic_counts[w, topic] += 1\n",
    "        doc_topic_counts[d, topic] += 1\n",
    "        topic_totals[topic] += 1\n",
    "        topics.append(topic)\n",
    "    doc_topic_assignments.append(topics)\n",
    "for it in range(n_iterations):\n",
    "    if it % 10 == 0:\n",
    "        print(f\"Done: {it}/{n_iterations} iterations\")\n",
    "    for d in range(n_docs):\n",
    "        doc = X[d].indices\n",
    "        for i, w in enumerate(doc):\n",
    "            current_topic = doc_topic_assignments[d][i]\n",
    "            word_topic_counts[w, current_topic] -= 1\n",
    "            doc_topic_counts[d, current_topic] -= 1\n",
    "            topic_totals[current_topic] -= 1\n",
    "\n",
    "            topic_probs = (word_topic_counts[w] + beta) * (doc_topic_counts[d] + alpha) / (topic_totals + beta * n_words)\n",
    "            topic_probs /= topic_probs.sum()\n",
    "\n",
    "            new_topic = np.random.choice(np.arange(n_topics), p=topic_probs)\n",
    "            word_topic_counts[w, new_topic] += 1\n",
    "            doc_topic_counts[d, new_topic] += 1\n",
    "            topic_totals[new_topic] += 1\n",
    "            doc_topic_assignments[d][i] = new_topic\n",
    "\n",
    "def get_top_words(word_topic_counts, vocab, n_top_words=10):\n",
    "    topics = []\n",
    "    for topic_idx in range(n_topics):\n",
    "        top_words_idx = word_topic_counts[:, topic_idx].argsort()[::-1][:n_top_words]\n",
    "        topics.append([vocab[i] for i in top_words_idx])\n",
    "    return topics\n",
    "\n",
    "topics = get_top_words(word_topic_counts, vocab)\n",
    "\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f\"Topic #{i + 1}: {', '.join(topic)}\")\n",
    "\n",
    "topic_assignments = np.argmax(doc_topic_counts, axis=1)\n",
    "topic_to_labels = {i: [] for i in range(n_topics)}\n",
    "\n",
    "for doc_idx, topic in enumerate(topic_assignments):\n",
    "    topic_to_labels[topic].append(labels[doc_idx])\n",
    "\n",
    "used_labels = set()\n",
    "final_topic_labels = {}\n",
    "for topic_idx, label_list in topic_to_labels.items():\n",
    "    label_counts = np.bincount(label_list, minlength=len(label_names))\n",
    "    if label_counts.sum() > 0:\n",
    "        for label_idx in label_counts.argsort()[::-1]:\n",
    "            if label_idx not in used_labels:\n",
    "                used_labels.add(label_idx)\n",
    "                final_topic_labels[topic_idx] = label_idx\n",
    "                break\n",
    "\n",
    "unused_labels = set(range(len(label_names))) - used_labels\n",
    "for topic_idx in range(n_topics):\n",
    "    if topic_idx not in final_topic_labels and unused_labels:\n",
    "        final_topic_labels[topic_idx] = unused_labels.pop()\n",
    "\n",
    "for topic_idx, label_idx in final_topic_labels.items():\n",
    "    print(f\"Topic #{topic_idx + 1}: {label_names[label_idx]} ({len(topic_to_labels[topic_idx])} documents)\")\n",
    "    print(f\"Top words: {', '.join(topics[topic_idx])}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
